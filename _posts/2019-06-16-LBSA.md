---
title: A Lexicon-Based Supervised Attention Model for Neural Sentiment Analysis
tags: Paper
---

## Contributions:

- Propose a novel lexicon-based attention model. It combines an attention mechanism with sentiment lexicons based on the sentiment degree.
- Benefiting from the guidance of supervised methods, our attention model has better interpretability and less noise than other attention models.
- Model outperforms state-of-the-art methods on three sentiment analysis datasets (IMDB, Yelp13, Yelp14).

## Approach

![lbsa](../../../../assets/LBSA.png)


We have a document $D$, which have $K$ Sentences$\{S_{1}, S_{2}, ..S_{k}\}$， for a sentence $S_{i}$ have $T$ words $\{w_{i 1}, w_{i 2}, ... w_{i t}\}$

#### 1. Gold Attention Vectors

Each word in the sentiment lexicon is annotated or assigned a fixed positive or negative score that reflects its sentiment polarity and strength.

##### 1.1 Word Level

We map all of them into the interval [-1; 1] as normalized sentiment scores, where a score in (0; 1] represents various scales of good and one close to 1 means very good, with negations showing the opposite.

$$
S D^{L}\left(w_{i t}\right)=\left|\operatorname{scor} e^{L}\left(w_{i t}\right)\right|
$$

- scoreL($w_{i t}$) is the normalized sentiment score according to lexicon L.
- SDL($w_{i t}$) is the absolute value of scoreL($w_{i t}$), which means its range is [0; 1].

$$
a_{i t}^{\star}=\frac{\exp \left(\lambda_{w} S D^{L}\left(w_{i t}\right)\right)}{\sum_{t=1}^{T} \exp \left(\lambda_{w} S D^{L}\left(w_{i t}\right)\right)}
$$

- $\lambda_{w}$ is a positive hyper-parameter that adjusts the variance of the sentiment degree.
- A larger $\lambda_{w}$ means a sharper distinction between sentiment terms and neutral ones in gold attention vectors.
- $a_{i t}^{\star}$ it denotes the tth dimension of $a_{i}^{\star}$.

##### 1.2 Sentence Level

$$
S D^{L}\left(S_{i}\right)=\frac{\sum_{t=1}^{T} S D^{L}\left(w_{i t}\right)}{T}
$$

$$
a_{i}^{\star}=\frac{\exp \left(\lambda_{s} S D^{L}\left(S_{i}\right)\right)}{\sum_{i=1}^{k} \exp \left(\lambda_{s} S D^{L}\left(S_{i}\right)\right)}
$$

#### 2. Learned Attention Vectors
##### 2.1 Word Level
$$
\pi_{i t}=\tanh \left(\mathbf{W}_{w f} \overrightarrow{\mathbf{h}_{i t}}+\mathbf{W}_{w b} \stackrel{\leftarrow}{\mathbf{h}_{i t}}+\mathbf{b}_{w}\right) \cdot \mathbf{v}_{w}^{\top}
$$

- $v_{w}$ is a weight vector that can record historical sentiment information
- $v_{w}$ is just like the query vector employed by memory networks
- $v_{w}$ is jointly learned during the training process

$$
a_{i t}=\frac{\exp \left(\pi_{i t}\right)}{\sum_{t=1}^{T} \exp \left(\pi_{i t}\right)}
$$

$$
\mathbf{s}_{i}=\sum_{t=1}^{T} a_{i t} \mathbf{h}_{i t}
$$

-  $ h_{i t} $ means a concatenation of $\overrightarrow{\mathbf{h_{i}}}$ and $\overleftarrow{\mathbf{h_{i}}}$


##### 2.2 Sentence Level

$$
\pi_{i}=\tanh \left(\mathbf{W}_{s f} \overrightarrow{\mathbf{h}}_{i}+\mathbf{W}_{s b} \stackrel{\leftarrow}{\mathbf{h}}_{i}+\mathbf{b}_{s}\right) \cdot \mathbf{v}_{s}^{\top}
$$

$$
a_{i}=\frac{\exp \left(\pi_{i}\right)}{\sum_{i=1}^{k} \exp \left(\pi_{i}\right)}
$$

$$
\mathbf{d}=\sum_{i=1}^{k} a_{i} \mathbf{h}_{i}
$$

- $d$, document

#### 3. Jointly Supervised Classification and Attention

$$
\mathbf{p}=\operatorname{softmax}\left(\mathbf{W}_{c} \mathbf{d}+\mathbf{b}_{c}\right)
$$

$$
\mathcal{L}=-\sum_{c=1}^{C} \mathbf{g}_{c} \log \left(\mathbf{p}_{c}\right)+\mu_{w} \cdot \sum_{i=1}^{T} \triangle\left(\mathbf{a}_{i}^{\star}, \mathbf{a}_{i}\right)+\mu_{s} \cdot \triangle\left(\mathbf{a}^{\star}, \mathbf{a}\right)
$$

- $\Delta$, loss fuction (cross entropy)
- $g_{C}$, the true classfication (one hot vector)
- $p_{C}$, the predict classfication
- $\mu_{w}$, $\mu_{s}$, the coefficients for attention loss functions, which can balance the preference between classification and attention disagreements, to alleviate the overfitting problem

$$
\triangle\left(\mathbf{a}^{\star}, \mathbf{a}\right)=-\sum_{i} \mathbf{a}_{i}^{\star} \log \left(\mathbf{a}_{i}\right)
$$

## Experiment
#### 1. Datasets and Lexicons
**Datasets :**
- IMDB
- Yelp 2013
- Yelp 2014

**Lexicons :**
- SentimentWordNet3.0 (SWN) (Baccianella et al., 2010)
- MPQA (Wilson et al., 2005)
- the Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013)
- the Hownet Knowledge Database (HKD)

#### 2. Hyper-parameter Settings

We set the dimensions of the word-level bidirectional LSTM hidden states to 100. All the weight matrices are initialized by a uniform distribution in [-0.1, 0.1]. The hyper-parameter $\lambda_{w}$ and $\lambda_{s}$ which adjust the variance of the gold attention vectors, are both set to 3.0. The hyper-parameter $\mu_{w}$ which adjusts the proportion of classification and attention loss, is set to 0.001 while $\mu_{s}$ is set to 0.05. We use Adam (Kingma and Ba, 2014) as the optimizer, which adopts a self-adaptive learning rate to optimize parameters, and its initial learning rate is set to 0.001. The batch size is set to 32 for efficiency. The model achieving best results on the developing set is chosen for the final evaluation of the test set.

#### 3. Result

![lbsaresult](../../../../assets/lbsaresult.png)

$$
Accuracy=\frac{T}{N}
$$
$$
R M S E=\sqrt{\frac{\sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}}{N}}
$$

#### 3.Limitation

The same word but different sentiment.

For the word **long**:
>*The several tables were available, so I didn’t understand the **long** wait.* [negative](#){:.button.button--primary.button--rounded.button--sm}

>*Its appeal to me was immediate and long lasting.* [positive](#){:.button.button--primary.button--rounded.button--sm}

Domain Words

![domainwords](../../../../assets/domainwords.png)

## Conclusion

It has better interpretability and less noise compared with other attention models.
