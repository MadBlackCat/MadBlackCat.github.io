---
title: Word2Vec 自己的理解
tags: NLP
---

## Introduction ##

**Two model variants:**

 - Skip-grams (SG)
 - Continuous Bag of Words (CBOW)

**Additional efficiency in training:**

 - Negative sampling
 - Hierarchical Softmax

## Distribution Similarity ##

通过word的context来表示这个word

通过这个词的同伴来指导这个词的意思

## Main Idea ##

1. We have a large corpus of text 

2. Every word is represented by a **One Hot** vector 

3. Go through the text, which has a center word *c* and context ("outside") words *o* 

4. Use the similarity of the word vectors for *c* and *o* to calculate the    probability of *o* given *c*

5. Maximize this probability（Negative-log-likehood ）

#### calculate the    probability of *o* given *c*

$$
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
$$

#### Object function

$$
Liklyhood = L(\theta)= \prod_{t=1}^{T} \prod_{-m \leq j \leq m} P\left(w_{t+j} | w_{t} ; \theta\right)
$$

$$
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)
$$



$$
\theta=\left[ \begin{array}{c}{v_{\text {aardvark}}} \\ {v_{a}} \\ {\vdots} \\ {v_{z e b r a}} \\ {u_{\text {aardvark}}} \\ {u_{a}} \\ {\vdots} \\ {u_{z e b r a}}\end{array}\right] \in \mathbb{R}^{2 d V}
$$
![图1](F:\Documents\documents\img\1558842324222.png)

   ![1558842536417](F:\Documents\documents\img\1558842536417.png)

#### Loss Function 

cross-entropy

#### Optimization

优化方法使用的是随机梯度下降 **Stochastic Gradient Descent**
mini-batch 的batch_size为1

## Continuous Bag of Words Model (CBOW) ##



![1558859017453](F:\Documents\documents\img\1558859017453.png)

#### Main Idea

一个浅层神经网络，输入一个context word的 one hot 向量， 得到它对应的center word 的 one hot 向量。



1. 输入context word 的one hot 向量， $\left(x^{(c-m)}, \ldots, x^{(c-1)}, x^{(c+1)}, \ldots, x^{(c+m)} \in \mathbb{R}^{|V|}\right)​$

2. 从输入到Hidden layer计算得到$ \left(v_{c-m}=\mathcal{V} x^{(c-m)}, v_{c-m+1}=\mathcal{V} x^{(c-m+1)}, \ldots, v_{c+m}=\mathcal{V} x^{(c+m)} \in \mathbb{R}^{n} \right) ​$

3. 计算这些向量的平均值， $\hat{\vartheta}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^{n}​$

4. 从hidden layer到output计算得到score $z=\mathcal{U} \hat{\vartheta} \in \mathbb{R}^{|V|}​$ 

5. 将 score转换成概率 $\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}​$

6. 与真实的one hot 向量 $y$ 作比较，使用cross entropy 作为 loss function 并计算

   ![1558862989533](F:\Documents\documents\img\1558862989533.png)

#### Loss Function

$$
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right)
$$

#### Minimize Object Function

$$
\begin{aligned} \text { minimize } J &=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\ &=-\log P\left(u_{c} | \hat{v}\right) \\ &=-\log \frac{\exp \left(u_{c}^{T} \hat{\vartheta}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{\vartheta}\right)} \\ &=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right) \end{aligned}
$$


## Skip-Gram ##

![1558843074086](F:\Documents\documents\img\1558843074086.png)



#### Main Idea ####

 	一个浅层神经网络，与CBOW完全相反，输入一个center word的 one hot 向量， 得到它对应的context word 的 one hot 向量。

1. 我们输入 x~k~（第 k 个单词的one hot向量）
2. 计算input 到 hidden $v_{c}=\mathcal{V} x$
3. 计算 hidden到 output $z=U v_{c}$
4. 将 score转换成概率 $\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$
5. 将输出与真实的one hot 向量作对比

![1558862514698](F:\Documents\documents\img\1558862514698.png)



#### Minimize Object Function

$$
\begin{aligned} \operatorname{minimize} J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \\ &=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \end{aligned}
$$

#### Loss Function

$$
\begin{aligned} J &=-\sum_{j=0, j \neq m}^{2 m} \log P\left(u_{c-m+j} | v_{c}\right) \\ &=\sum_{j=0, j \neq m}^{2 m} H\left(\hat{y}, y_{c-m+j}\right) \end{aligned}
$$

## Negative Sampling

随机采样一些negative words



![1558863541559](F:\Documents\documents\img\1558863541559.png)


$$
\theta=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in D} P(D=0 | w, c, \theta)
\\
=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}}(1-P(D=1 | w, c, \theta))\\
=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in D} \log (1-P(D=1 | w, c, \theta))\\
=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right)\\
=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)
$$

#### Object function

$\tilde{D}$ 是negative words， $Z$ 是扩大系数， $U$ 是词频 

$$
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \overline{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)
$$


$$
\begin{array}{c}{U\left(w_{i}\right)=\frac{\operatorname{connt}\left(w_{i}\right)}{\sum_{i} \operatorname{count}\left(w_{i}\right)}} \\ {P_{n}\left(w_{i}\right)=\frac{U\left(w_{i}\right)^{\frac{3}{4}}}{Z}}\end{array}
$$






## Hierarchical Softmax

使用霍夫曼树降低算法复杂度
遍历树的节点，左走和右走，将相应概率乘在一起，节点就是单词

$$
P\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)
$$

$$
[x]=\left\{\begin{array}{l}{1 \text { if } x \text { is true }} \\ {-1 \text { otherwise }}\end{array}\right.
$$

单词概率

$$
\begin{aligned} P\left(w_{2} | w_{i}\right) &=p\left(n\left(w_{2}, 1\right), \text { left }\right) \cdot p\left(n\left(w_{2}, 2\right), \text { left }\right) \cdot p\left(n\left(w_{2}, 3\right), \text { right) }\right.\\ &=\sigma\left(v_{n\left(w_{2}, 1\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(v_{n\left(w_{2}, 2\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(-v_{n\left(w_{2}, 3\right)}^{T} v_{w_{i}}\right) \end{aligned}
$$

其他的都和之前一样，最小化 $-\log P\left(w | w_{i}\right)$